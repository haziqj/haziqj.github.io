[{"authors":null,"categories":null,"content":"I am an Assistant Professor in Statistics at Universiti Brunei Darussalam, the leading higher education institution in Brunei. My research interests lie in statistical theory, methods and computation, with a special inclination towards social science applications.\nI obtained my PhD in Statistics from the London School of Economics and Political Science (LSE) in October 2018. My PhD project explored the use of Fisher information-dependent priors in a vector space framework for regression, classification, and variable selection. My supervisors were Dr. Wicher Bergsma and Prof. Irini Moustaki. I also obtained an MSc in Statistics from LSE in 2014.\nI graduated from Warwick University, completing the 4-year MMORSE degree in 2010. As my final year project, I used Bradley-Terry models applied to English Premier League football data. This project was supervised by Prof. David Firth.\nPreviously, I was a Research Officer at the Centre of Science \u0026amp; Technology, Research \u0026amp; Development (CSTRAD), Ministry of Defence, Brunei. My primary task was to provide data analysis and decision support to strategic acquisition projects, and assist in statistical analyses for defence-related research.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1631006475,"objectID":"4be3b5c569b714876b11c3c9391957b9","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am an Assistant Professor in Statistics at Universiti Brunei Darussalam, the leading higher education institution in Brunei. My research interests lie in statistical theory, methods and computation, with a special inclination towards social science applications.","tags":null,"title":"Haziq Jamil","type":"authors"},{"authors":null,"categories":null,"content":"I will be teaching SM4202 for the first half of the semester (roughly 7 weeks) for the August 2020 semester.\nSyllabus Here are the topics I intend to cover for this course. This may change depending on how we get on with the course.\n  Chapter 1: Probability recap\n Recall fundamental concepts in mathematical probability Recap of discrete and continuous distributions Properties of means and variances Generating functions     Chapter 2: Discrete time stochastic processes\n Definitions and examples Discrete Markov chains Transition probabilities, stationary transition probabilities, $n$-step transition matrices Chapman-Kolmogorov equations State diagrams and classification of states Periodicity, recurrence and transience     Chapter 3: Continuous time stochastic processes\n Basic concepts and definitions Instantaneous transition rates $Q$ matrices (generator matrices) Kolmogorov equations Invariant measures for irreducible, continuous Markov processes     Schedule  W4 Tue 18/8/2020 1330-1530 @ 1A.62; Lecture (Chapter 1) W4 Wed 19/8/2020 1330-1530 @ 1A.62; Lecture (Chapter 1) W5 Tue 25/8/2020 1300-1530 @ 1A.62; Lecture (Chapter 2) W5 Wed 26/8/2020 1230-1400 @ 1A.62; Lecture (Chapter 2) W5 Sat 29/8/2020 0800-1000 @ 1A.62; Tutorial (Exercise 1) W6 Tue 01/9/2020 1330-1530 @ 1A.62; Lecture (Chapter 2) W6 Wed 02/9/2020 1330-1530 @ 1A.62; Lecture (Chapter 2) W6 Sat 05/9/2020 0800-1000 @ 1A.62; Tutorial (Exercise 2) W7 Tue 08/9/2020 1330-1530 @ 1A.62; Lecture (Chapter 3) W7 Wed 09/9/2020 1330-1530 @ 1A.62; Lecture (Chapter 3) MIDSEM Mon 14/9/2020 0800-1100 @ 1A.62; Lecture and Tutorial (Chapter 3 \u0026amp; Exercise 3)   CLASS TEST is scheduled for Sat 26/9/2020 @ 0830-0930. Covers Chapters 1 and 2 only.   Lecture Notes Download lecture notes from my shared Google Drive.\nExercise Sheets  Exercise 1 / Solutions Exercise 2 / Solutions Exercise 3 / Solutions  Statistical Tables  Link  Supplementary Reading  Ross, S. (2009). A First Course in Probability. Ross, S. M. (2014). Introduction to probability models. Academic Press. Grimmet, G. \u0026amp; Stirzaker, D. (2020). Probability and Random Processes. Oxford University Press. Norris, J. R. (1998). Markov Chains. Cambridge University Press.  Other Links  Epsilon-Delta Definition of Continuity Simulating an Epidemic (3B1B)  ","date":1596585600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1631006475,"objectID":"fd0c9beea1be234b0d071bd85b3871ec","permalink":"https://haziqj.ml/teaching/sm4202utb/","publishdate":"2020-08-05T00:00:00Z","relpermalink":"/teaching/sm4202utb/","section":"teaching","summary":"UTB School of Applied Sciences and Mathematics module (10 credits). This module covers stochastic processes through a wide range of applications that will develop probabilistic intuition.","tags":null,"title":"SM4202 Random Processes","type":"book"},{"authors":null,"categories":null,"content":"General Please download RStudio onto your laptops. Install the tidyverse package by running install.packages(\u0026quot;tidyverse\u0026quot;) in the R terminal. You may like to follow along during class as I present the R codes, so bring along your laptops to class if you wish.\nSchedule  Mon 19/10/2020 1000-1200 ONLINE; Exploratory Data Analysis Mon 26/10/2020 1000-1200 ONLINE; Hypothesis Testing Mon 9/11/2020 1000-1200 ONLINE; Linear Regression  Lecture Slides  Lecture 1 Lecture 2 Lecture 3  Resources  Titanic data set Poverty data set  ","date":1580947200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1631006475,"objectID":"17d915586a439e835f95db81be0d59f3","permalink":"https://haziqj.ml/teaching/sr5101/","publishdate":"2020-02-06T00:00:00Z","relpermalink":"/teaching/sr5101/","section":"teaching","summary":"UBD Masters Module\u0026mdash;This module is designed to provide new graduate students involved in research in the sciences with the skills and resources needed for successful research.","tags":null,"title":"SR-5101 Advanced Research Skills","type":"book"},{"authors":null,"categories":null,"content":" HTML PDF  ","date":1559865600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1631006475,"objectID":"141a8a297a9a85ec87a90adf81618260","permalink":"https://haziqj.ml/teaching/linearregression/","publishdate":"2019-06-07T00:00:00Z","relpermalink":"/teaching/linearregression/","section":"teaching","summary":"Learn the basics of linear models.","tags":null,"title":"Crash Course in Linear Regression","type":"book"},{"authors":null,"categories":null,"content":"  -- Table of Contents  Lectures  Lecture 1: Introduction to the Data Science Framework Lecture 2: Introduction to R Lecture 3: Data Science with R Pt. 1 Lecture 4: Data Science with R Pt. 2   Assignment    -- The aims of this short course are the following:\n To become familiar with the data science framework. To become familiar with the R programming language and the Integrated Development Environment (IDE) RStudio. To be able to import, summarise and visualise data in RStudio. To complete and report on an exploratory data analysis of the Kiva.org global loans dataset.  The course is delivered in the form of lecture-style presentation, with hands-on and interactive R sessions, so please bring your laptops with you (and have RStudio pre-installed). Four 2-hour lectures are planned (see below for details of each lecture). There is one assignment for this course, and there are mini exercises at the end of lectures.\nThis page will be updated as and when the material becomes ready.\n Please download R and RStudio Desktop before attending the lectures.   Lectures Lecture 1: Introduction to the Data Science Framework Slides Resources:\n Machine Learning and Data Science talk by Neil Lawrence R for Data Science Data Science Wiki page NEW Inference vs Prediction @ Data Science Blog NEW Inference vs Prediction @ Stack Exchange NEW Bias-Variance tradeoff @ Towards Data Science NEW Bias-Variance tradeoff @ Scott Fortmann  Lecture 2: Introduction to R Notes R Exercise Resources:\n Hands on Programming with R R Packages  Lecture 3: Data Science with R Pt. 1 Walk through of Chapters 2\u0026ndash;3 of R for Data Science book.\nLecture 4: Data Science with R Pt. 2 Notes Walk through of Chapters 4\u0026ndash;8 of R for Data Science book.\nAssignment CSTRAD July 2019 class leaders:\n Team A: Vincent Team B: Wen Jei  Due on 2nd August 2019 12.00pm.\n Data Science Framework This section to be updated.\n  Getting Started with R Key ideas Objects R works on objects. All objects have the following properties (referred to as intrinsic attributes): mode: tells us what kind of thing the object is – possible modes include numeric, complex, logical, character and list.\n  R data science functions As usual, before starting, load all the packages you need. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.\n  Assignment Assignment You are tasked with conducting an exploratory data analysis of the Kiva.org loans data set. In particular, focus on the following tasks: Investigate the relationship between the loan taker\u0026rsquo;s ability to pay and the usage of the loan (what it was funded for).\n  ## Meet your instructor Haziq Jamil ## FAQs Are there prerequisites? There are no prerequisites for the first course.\n How often do the courses run? Continuously, at your own pace.\n  Begin the course   -- ","date":1559865600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1631006475,"objectID":"e97db74d5e4b05828d886b8e74c6b4e0","permalink":"https://haziqj.ml/teaching/datascience/","publishdate":"2019-06-07T00:00:00Z","relpermalink":"/teaching/datascience/","section":"teaching","summary":"Learn about the data science framework, including importing, summarising, and visualising data using R.","tags":null,"title":"Introductory Data Science using R","type":"book"},{"authors":null,"categories":null,"content":"This section to be updated.\n","date":1562281200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"f41a1db61e9db11454d74c0cde17b86f","permalink":"https://haziqj.ml/teaching/datascience/1-intro/","publishdate":"2019-07-05T00:00:00+01:00","relpermalink":"/teaching/datascience/1-intro/","section":"teaching","summary":"This section to be updated.","tags":null,"title":"Data Science Framework","type":"book"},{"authors":null,"categories":null,"content":"Key ideas Objects R works on objects. All objects have the following properties (referred to as intrinsic attributes):\n mode: tells us what kind of thing the object is – possible modes include numeric, complex, logical, character and list. length: is the number of components that make up the object.  At the simplest level, an object is a convenient way to store information. In statistics, we need to store observations of a variable of interest. This is done using a numeric vector. Note that there are no scalars in R; a number is just a numeric vector of length 1. Vectors are referred to as atomic structures; all of their components have the same mode.\nIf an object stores information, we need to name it so that we can refer to it later (and thus recover the information that it contains). The term used for the name of an object is identifier. An identifier is something that we choose. Identifiers can be chosen fairly freely in R. The points below are a few simple rules to bear in mind.\n In general any combination of letters, digits and the dot character can be used although it is obviously sensible to choose names that are reasonably descriptive. You cannot start an identifier with a digit or a dot so moonbase3.sample is acceptable but 3moons.samplebase and .sample3basemoon are not. Identifiers are CASE SENSITIVE so moon.sample is different from moon.Sample. It is easy to get caught out by this. Some characters are already assigned values. These include c, q, t, C, D, F, I and T. Avoid using these as identifiers.  Typically we are interested in data sets that consist of several variables. In R, data sets are represented by an object known as a data frame. As with all objects, a data frame has the intrinsic attributes mode and length; data frames are of mode list and the length of a data frame is the number of variables that is contains. In common with many larger objects, a data frame has other attributes in addition to mode and length. The non-intrinsic attributes of a data frame are:\n names: these are the names of the variables that make up the data set; row.names: these are the names of the individuals on whom the observations are made; class: this attribute can be thought of as a detailed specification of the kind of thing the object is; in this case the class is data.frame.  The class attribute tells certain functions (generic functions) how to deal with the object. For example, objects of class \u0026ldquo;data.frame\u0026rdquo; are displayed on screen in a particular way.\nFunctions, arguments and return values R works by calling functions. The notion of a function is a familiar one; a function takes a collection of inputs and maps them to a single output. In R, inputs and output are objects. The inputs are referred to as arguments and the output is referred to as the return value. Many useful functions are part of the standard R setup. Others are available via the inclusion of packages. One of the key advantages of R is the ease with which users can define their own functions. Functions are also objects; the mode of a function is function (sensibly enough). In computing, functions have side-effects. For example, calling a function may cause a graph to be drawn. In many instances, it is the side-effect that is important rather than the return value.\nWorkspace and working directories During an R session, a number of objects will be generated; for example we may generate vectors, data frames and functions. For the duration of the session, these objects are stored in an area of memory referred to as the workspace. If we want to save the objects for future use, we instruct R to write them to a file in our current working directory (directory is just another name for a folder). Note the distinction: things in memory are temporary (they will be lost when we log out); files are more permanent (they are stored on disk and the information they contain can be loaded into memory during our next session). Managing objects and files is an important part of using R effectively.\nA new R session Starting up If you haven\u0026rsquo;t already, download R and RStudio based on your operating system. R can actually be run in the terminal (type r in your terminal) or the original R program itself.\nWe will be working in RStudio, which is a user-friendly integrated development environment (IDE).\nIt is recommended to create a project, which automatically configures the working directory. To do this, go to $\\text{File} \\rightarrow \\text{New project\u0026hellip;}$. If your files are already in a folder, select $\\text{Existing Directory}$; otherwise select $\\text{New Directory}$. Don\u0026rsquo;t select $\\text{Version Control}$ for now.\nUsing R as a calculator The simplest thing that R can do is evaluate arithmetic expressions.\n1 + 1 ## [1] 2 1 + 4.23 ## [1] 5.23 1 + 1/2 * 9 - 3.14 ## [1] 2.36 # Note the order in which operations are performed in the final calculation   Comments in R. R ignores anything after a # sign in a command. We will follow this convention. Anything after a # in a set of R commands is a comment.\n Vectors and assignment We can create vectors at the command prompt using the concatenation function c(...). c(object1,object2,...).\nThis function takes arguments of the same mode and returns a vector containing these values.\nc(1,2,3) ## [1] 1 2 3 c(\u0026quot;Alpha\u0026quot;, \u0026quot;Bravo\u0026quot;, \u0026quot;Charlie\u0026quot;) ## [1] \u0026quot;Alpha\u0026quot; \u0026quot;Bravo\u0026quot; \u0026quot;Charlie\u0026quot;  In order to make use of vectors, we need identifiers for them (we do not want to have to write vectors from scratch every time we use them). This is done using the assignment operator \u0026lt;- via name \u0026lt;- expression. name now refers to an object whose value is the result of evaluating expression.\nnumbers \u0026lt;- c(1,2,3) people \u0026lt;- c(\u0026quot;Alpha\u0026quot;, \u0026quot;Bravo\u0026quot;, \u0026quot;Charlie\u0026quot;) numbers ## [1] 1 2 3 people ## [1] \u0026quot;Alpha\u0026quot; \u0026quot;Bravo\u0026quot; \u0026quot;Charlie\u0026quot; # Typing an object’s identifier causes R to print the contents of the object  Simple arithmetic operations can be performed with vectors.\n1:3 + c(4, 5, 6) ## [1] 5 7 9 numbers + numbers ## [1] 2 4 6 numbers - c(8, 7.5, -2) ## [1] -7.0 -5.5 5.0 rep(12, 3) / numbers ## [1] 12 6 4  Note in the above example that multiplication and division are done element by element.\n Reusing commands. If you want to bring back a command which you have used earlier in the session, press the up arrow key ↑. This allows you to go back through the commands until you find the one you want. The commands reappear at the command line and can be edited and then run by pressing return.\n The outcome of an arithmetic calculation can be given an identifier for later use.\ncalc1 \u0026lt;- numbers + c(8, 7.5, -2) calc2 \u0026lt;- calc1 * calc1 calc1 ## [1] 9.0 9.5 1.0 calc2 ## [1] 81.00 90.25 1.00 (calc1 \u0026lt;- calc1 + calc2) ## [1] 90.00 99.75 2.00 calc1 ## [1] 90.00 99.75 2.00 calc2 ## [1] 81.00 90.25 1.00 # Note: in the final step we have update the value of calc1 by adding calc2 to # the old value; calc1 changes but calc2 is unchanged  If we try to add together vectors of different lengths, R uses a recycling rule; the smaller vector is repeated until the dimensions match.\n1:5 + 1 ## [1] 2 3 4 5 6 small \u0026lt;- c(1, 2) large \u0026lt;- c(0, 0, 0, 0, 0, 0) large + small ## [1] 1 2 1 2 1 2   Where have all the objects gone? We have defined a lot of variables so far. Inspect the \u0026lsquo;Environment\u0026rsquo; tab to inspect all objects saved in the workspace.\n Example: sheep weight We have taken a random sample of the weight of 5 sheep in the UK. The weights (kg) are 84.5, 72.6, 75.7, 94.8, and 71.3. We are going to put these values in a vector and illustrate some standard procedures.\nweight \u0026lt;- c(84.5, 72.6, 75.7, 94.8, 71.3) weight ## [1] 84.5 72.6 75.7 94.8 71.3 total \u0026lt;- sum(weight) numobs \u0026lt;- length(weight) meanweight \u0026lt;- total / numobs meanweight ## [1] 79.78 # We have worked out the mean the hard way. There is a quick way... mean(weight) ## [1] 79.78  Matrices Another form of data in R are matrices. A matrix is formed using the matrix() function, which takes in a vector argument, plus the number of rows and number of columns the matrix should be. For example, to create a $4 \\times 3$ matrix, one types\nmat \u0026lt;- matrix(1:12, nrow = 4, ncol = 3) mat ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12  Notice that the matrix is built column by column. If we wish to specify that the matrix be built by filling in the rows first, we use the option byrow = TRUE.\nmat \u0026lt;- matrix(1:12, nrow = 4, byrow = TRUE) mat ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 ## [4,] 10 11 12  In the above, we skipped the ncol argument as it is not needed. If it is unambiguous enough, then you can skip one of nrow or ncol, but it is certainly good practice to specify both in order to avoid silly mistakes.\nRemember, R practices the recycling rule, and this applies even to matrices. The following code will not given an error, but it may not have the intended result.\nmat \u0026lt;- matrix(1:3, nrow = 10, ncol = 3, byrow = TRUE) mat ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 1 2 3 ## [3,] 1 2 3 ## [4,] 1 2 3 ## [5,] 1 2 3 ## [6,] 1 2 3 ## [7,] 1 2 3 ## [8,] 1 2 3 ## [9,] 1 2 3 ## [10,] 1 2 3  A very useful class of matrices that you would often encounter is diagonal matrices. These are built using the diag() function in R.\ndiag(3) # the 3 x3 identity matrix ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 diag(1:5) # a diagonal matrix with diagonal entries 1, 2, ..., 5 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 0 0 0 0 ## [2,] 0 2 0 0 0 ## [3,] 0 0 3 0 0 ## [4,] 0 0 0 4 0 ## [5,] 0 0 0 0 5  Data frames A data frame is an R object that can be thought of as representing a data set. A data frame consists of variables (columns vectors) of the same length with each row corresponding to an experimental unit. The general syntax for setting up a data frame is name \u0026lt;- data.frame(variable1, variable2, ...).\nIndividual variables in a data frame are accessed using the $ notation: name$variable.\nOnce a data frame has been created we can view and edit it in a spreadsheet format using the command fix(...) (or equivalently data.entry(...)). New variables can be added to an existing data frame by assignment.\nExample: sheep again Suppose that, for each of the sheep weighed in the example above, we also measure the height at the shoulder. The heights (cm) are 86.5, 71.8, 77.2, 84.9, and 75.4. We will set up another variable for height. We would also like to have a single structure in which the association between weight and height (that is, that they are two measurements of the same sheep) is made explicit. This is done by adding each variable to a dataframe. We will call the data frame sheep and view it using fix(sheep).\nheight \u0026lt;- c(86.5, 71.8, 77.2, 84.9, 75.4) sheep \u0026lt;- data.frame(weight, height) mean(sheep$height) ## [1] 79.16 ## ## fix(sheep)  Suppose that a third variable consisting of measurements of the length of the sheeps’ backs becomes available. The values (in cm) are 130.4, 100.2, 109.4, 140.6, and 101.4. We can include a new variable in the data frame using assignment. Suppose we choose the identifier backlength for this new variable:\nsheep$backlength \u0026lt;- c(130.4, 100.2, 109.4, 140.6, 101.4)  Look at the data in spreadsheet format to check what has happened.\nLists Lists in R are objects which can hold various kinds of objects together in one list. The easiest way to explain this is to construct a list:\nmy_list \u0026lt;- list( a = 1, b = diag(1:3), c = lm(rnorm(100) ~ 1 + rt(100, 1)), d = plot(rnorm(10), ), e = sheep ) str(my_list) ## List of 5 ## $ a: num 1 ## $ b: int [1:3, 1:3] 1 0 0 0 2 0 0 0 3 ## $ c:List of 12 ## ..$ coefficients : Named num [1:2] -0.13641 0.00289 ## .. ..- attr(*, \u0026quot;names\u0026quot;)= chr [1:2] \u0026quot;(Intercept)\u0026quot; \u0026quot;rt(100, 1)\u0026quot; ## ..$ residuals : Named num [1:100] 1.814 -0.203 0.631 -0.449 -2.427 ... ## .. ..- attr(*, \u0026quot;names\u0026quot;)= chr [1:100] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; ... ## ..$ effects : Named num [1:100] 1.389 1.013 0.465 -0.614 -2.592 ... ## .. ..- attr(*, \u0026quot;names\u0026quot;)= chr [1:100] \u0026quot;(Intercept)\u0026quot; \u0026quot;rt(100, 1)\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; ... ## ..$ rank : int 2 ## ..$ fitted.values: Named num [1:100] -0.136 -0.139 -0.135 -0.138 -0.137 ... ## .. ..- attr(*, \u0026quot;names\u0026quot;)= chr [1:100] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; ... ## ..$ assign : int [1:2] 0 1 ## ..$ qr :List of 5 ## .. ..$ qr : num [1:100, 1:2] -10 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ... ## .. .. ..- attr(*, \u0026quot;dimnames\u0026quot;)=List of 2 ## .. .. .. ..$ : chr [1:100] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; ... ## .. .. .. ..$ : chr [1:2] \u0026quot;(Intercept)\u0026quot; \u0026quot;rt(100, 1)\u0026quot; ## .. .. ..- attr(*, \u0026quot;assign\u0026quot;)= int [1:2] 0 1 ## .. ..$ qraux: num [1:2] 1.1 1 ## .. ..$ pivot: int [1:2] 1 2 ## .. ..$ tol : num 1e-07 ## .. ..$ rank : int 2 ## .. ..- attr(*, \u0026quot;class\u0026quot;)= chr \u0026quot;qr\u0026quot; ## ..$ df.residual : int 98 ## ..$ xlevels : Named list() ## ..$ call : language lm(formula = rnorm(100) ~ 1 + rt(100, 1)) ## ..$ terms :Classes 'terms', 'formula' language rnorm(100) ~ 1 + rt(100, 1) ## .. .. ..- attr(*, \u0026quot;variables\u0026quot;)= language list(rnorm(100), rt(100, 1)) ## .. .. ..- attr(*, \u0026quot;factors\u0026quot;)= int [1:2, 1] 0 1 ## .. .. .. ..- attr(*, \u0026quot;dimnames\u0026quot;)=List of 2 ## .. .. .. .. ..$ : chr [1:2] \u0026quot;rnorm(100)\u0026quot; \u0026quot;rt(100, 1)\u0026quot; ## .. .. .. .. ..$ : chr \u0026quot;rt(100, 1)\u0026quot; ## .. .. ..- attr(*, \u0026quot;term.labels\u0026quot;)= chr \u0026quot;rt(100, 1)\u0026quot; ## .. .. ..- attr(*, \u0026quot;order\u0026quot;)= int 1 ## .. .. ..- attr(*, \u0026quot;intercept\u0026quot;)= int 1 ## .. .. ..- attr(*, \u0026quot;response\u0026quot;)= int 1 ## .. .. ..- attr(*, \u0026quot;.Environment\u0026quot;)=\u0026lt;environment: R_GlobalEnv\u0026gt; ## .. .. ..- attr(*, \u0026quot;predvars\u0026quot;)= language list(rnorm(100), rt(100, 1)) ## .. .. ..- attr(*, \u0026quot;dataClasses\u0026quot;)= Named chr [1:2] \u0026quot;numeric\u0026quot; \u0026quot;numeric\u0026quot; ## .. .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr [1:2] \u0026quot;rnorm(100)\u0026quot; \u0026quot;rt(100, 1)\u0026quot; ## ..$ model :'data.frame':\t100 obs. of 2 variables: ## .. ..$ rnorm(100): num [1:100] 1.678 -0.343 0.497 -0.586 -2.564 ... ## .. ..$ rt(100, 1): num [1:100] 0.157 -1.019 0.578 -0.404 -0.263 ... ## .. ..- attr(*, \u0026quot;terms\u0026quot;)=Classes 'terms', 'formula' language rnorm(100) ~ 1 + rt(100, 1) ## .. .. .. ..- attr(*, \u0026quot;variables\u0026quot;)= language list(rnorm(100), rt(100, 1)) ## .. .. .. ..- attr(*, \u0026quot;factors\u0026quot;)= int [1:2, 1] 0 1 ## .. .. .. .. ..- attr(*, \u0026quot;dimnames\u0026quot;)=List of 2 ## .. .. .. .. .. ..$ : chr [1:2] \u0026quot;rnorm(100)\u0026quot; \u0026quot;rt(100, 1)\u0026quot; ## .. .. .. .. .. ..$ : chr \u0026quot;rt(100, 1)\u0026quot; ## .. .. .. ..- attr(*, \u0026quot;term.labels\u0026quot;)= chr \u0026quot;rt(100, 1)\u0026quot; ## .. .. .. ..- attr(*, \u0026quot;order\u0026quot;)= int 1 ## .. .. .. ..- attr(*, \u0026quot;intercept\u0026quot;)= int 1 ## .. .. .. ..- attr(*, \u0026quot;response\u0026quot;)= int 1 ## .. .. .. ..- attr(*, \u0026quot;.Environment\u0026quot;)=\u0026lt;environment: R_GlobalEnv\u0026gt; ## .. .. .. ..- attr(*, \u0026quot;predvars\u0026quot;)= language list(rnorm(100), rt(100, 1)) ## .. .. .. ..- attr(*, \u0026quot;dataClasses\u0026quot;)= Named chr [1:2] \u0026quot;numeric\u0026quot; \u0026quot;numeric\u0026quot; ## .. .. .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr [1:2] \u0026quot;rnorm(100)\u0026quot; \u0026quot;rt(100, 1)\u0026quot; ## ..- attr(*, \u0026quot;class\u0026quot;)= chr \u0026quot;lm\u0026quot; ## $ d: NULL ## $ e:'data.frame':\t5 obs. of 3 variables: ## ..$ weight : num [1:5] 84.5 72.6 75.7 94.8 71.3 ## ..$ height : num [1:5] 86.5 71.8 77.2 84.9 75.4 ## ..$ backlength: num [1:5] 130 100 109 141 101  In the above example, my_list is an R list which contains 5 entries. The first entry called a is simply a number (or a vector of length 1). The second entry called b is a matrix. The third entry called c is a linear model. The fourth entry called d is a plot. Finally the fifth entry called e is a data frame. A list may be named (e.g. a to e above), or they need not be named.\nWhat is clear is that a list is able to hold not just one type of object, but various kinds of objects. This makes it useful to hold information or results for retrieval later, or for saving. Complex functions usually output a list rather than just singular objects.\nSubsetting In R, subsetting is done using square brackets:\na \u0026lt;- 1:10 a[3] # subset the 3rd element in the vector ## [1] 3 a[1:5] # subset the first 5 elements ## [1] 1 2 3 4 5 a[a %% 2 == 0] # subset even elements only (note: the '%%' operator gives the remainder) ## [1] 2 4 6 8 10  It is also possible to index matrices and dataframes\nmat[2, 3] # get the (2,3) element from the matrix ## [1] 3 sheep[1, 2] # get the first element in the 2nd column ## [1] 86.5  For data frames, you\u0026rsquo;ve seen that it is possible to use the $ symbol to get the columns. From this, it is vectorised so it\u0026rsquo;s further possible to subset using [].\nsheep$height[1] # the first sheep's height ## [1] 86.5  R management The R help system There are a number of different ways of getting help in R.\n If you have a query about a specific function then typing ? and then the functions name at the prompt will bring up the relevant help page. If your problem is of a more general nature, then typing help.start() will open up a window which allows you to browse for the information you want. The search engine on this page is very helpful for finding context specific information.  Alternatively, click on the \u0026lsquo;Help\u0026rsquo; tab and search for a term that you would like help on.\nSession management and visibility All of the objects created during an R session are stored in a workspace in memory. We can see the objects that are currently in the workspace by using the command objects(). Notice the (), these are vital for the command to work.\nobjects() ## [1] \u0026quot;a\u0026quot; \u0026quot;calc1\u0026quot; \u0026quot;calc2\u0026quot; \u0026quot;delete_files\u0026quot; \u0026quot;height\u0026quot; ## [6] \u0026quot;large\u0026quot; \u0026quot;mat\u0026quot; \u0026quot;meanweight\u0026quot; \u0026quot;my_list\u0026quot; \u0026quot;numbers\u0026quot; ## [11] \u0026quot;numobs\u0026quot; \u0026quot;people\u0026quot; \u0026quot;sheep\u0026quot; \u0026quot;small\u0026quot; \u0026quot;total\u0026quot; ## [16] \u0026quot;weight\u0026quot;  The information in the variables height and weight is currently encapsulated in the data frame sheep. We can tidy up our workspace by removing the height and weight variables (and various others that we are no longer interested in) using the rm() function. Do this and then check what is left.\nrm(height, weight, meanweight, numobs, total) objects() ## [1] \u0026quot;a\u0026quot; \u0026quot;calc1\u0026quot; \u0026quot;calc2\u0026quot; \u0026quot;delete_files\u0026quot; \u0026quot;large\u0026quot; ## [6] \u0026quot;mat\u0026quot; \u0026quot;my_list\u0026quot; \u0026quot;numbers\u0026quot; \u0026quot;people\u0026quot; \u0026quot;sheep\u0026quot; ## [11] \u0026quot;small\u0026quot;  The height and weight variables are now only accessible via the sheep data frame.\n## weight # should get an error message sheep$weight ## [1] 84.5 72.6 75.7 94.8 71.3  We can save the current workspace to file at any time. The command to do so is save.image(). However, RStudio makes it simple to save the current workspace. Since we have created a project, workspace saving is handled for us, and we are prompted to save when we quit RStudio or close the project.\nTo inspect our working directory, use the dir() command. We can also look at the files under the \u0026lsquo;Files\u0026rsquo; tab.\nWriting your own functions Every function in R has three basic parts: a name, a body of code, and a set of arguments. To make your own function, you need to replicate these parts and store them in an R object, which you can do with the function() function. To do this, call function() and follow it with a pair of braces, {}:\nmy_fun \u0026lt;- function(what.to.print = NULL) { if (is.null(what.to.print)) { print(\u0026quot;Hello world!\u0026quot;) } else { print(what.to.print) } }  Here is a simple function to print some text to the R console. Try it out:\nmy_fun()  ## [1] \u0026quot;Hello world!\u0026quot;  my_fun(\u0026quot;All your base are belong to us\u0026quot;)  ## [1] \u0026quot;All your base are belong to us\u0026quot;  Installing packages R has a very diverse user-contributed package repository called the Comprehensive R Archive Network (CRAN). Packages are nothing more than a collection of user-written functions that serve a particular purpose. For example, there are packages for extending the graphics capabilities of R (ggplot2), random-effects modelling (lme4), latent variable modelling (lavaan), adn so on. These aim to supplement base R with additional functionality.\n\u0026ldquo;Official\u0026rdquo; packages are downloaded from CRAN and installed by users all over the world. Packages that go are published on CRAN have gone through the necessary checks to ensure that they do not break when installed by R users.\nTo install packages from CRAN, the command is install.packages(). It is much easier, however, to use the GUI in RStudio to install packages. Hit the \u0026lsquo;Packages\u0026rsquo; tab, click install, then search for the package that you want.\nPackages do not necessarily have to be installed from CRAN. You can create an R package yourself and distributed either in compressed form via e-mail or USB stick, or host it on a version-control software development site like GitHub. If you would like to know more about creating your own packages, read this book (R Packages by Hadley Wickham).\nLoading packages to be used Installing packages makes them available for use, but does not load them when you start R. Every time you quit an R session and you start it back up again, you would have to load packages that you need all over again.\nThe command to load packages is library(\u0026lt;pkg name\u0026gt;). For example\nlibrary(tidyverse)  ## ── Attaching packages ───────────────────────────────────────────────────────────── tidyverse 1.3.0 ──  ## ✓ ggplot2 3.3.2 ✓ purrr 0.3.4 ## ✓ tibble 3.0.3 ✓ dplyr 1.0.0 ## ✓ tidyr 1.1.0 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.5.0  ## ── Conflicts ──────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag()  ","date":1562601600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"2b57d90671a2868387173fa0904650e0","permalink":"https://haziqj.ml/teaching/datascience/2-intro-to-r/","publishdate":"2019-07-09T00:00:00+08:00","relpermalink":"/teaching/datascience/2-intro-to-r/","section":"teaching","summary":"Key ideas Objects R works on objects. All objects have the following properties (referred to as intrinsic attributes):\n mode: tells us what kind of thing the object is – possible modes include numeric, complex, logical, character and list.","tags":null,"title":"Getting Started with R","type":"book"},{"authors":null,"categories":null,"content":"As usual, before starting, load all the packages you need.\nlibrary(tidyverse)  ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──  ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.1.4 ✓ dplyr 1.0.7 ## ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ## ✓ readr 2.0.1 ✓ forcats 0.5.1  ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag()  Data management Importing data sets Data can be imported by going to $\\text{File}\\rightarrow\\text{Import Dataset}$. Alternatively, the code is\n# code for importing data  Converting to tibbles as_tibble(iris)  ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # … with 140 more rows  Subsetting To extract columns, use the $ symbol.\niris$Sepal.Length  ## [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7 5.4 5.1 ## [19] 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4 5.2 5.5 4.9 5.0 ## [37] 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6 5.3 5.0 7.0 6.4 6.9 5.5 ## [55] 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1 5.6 6.7 5.6 5.8 6.2 5.6 5.9 6.1 ## [73] 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7 5.5 5.5 5.8 6.0 5.4 6.0 6.7 6.3 5.6 5.5 ## [91] 5.5 6.1 5.8 5.0 5.6 5.7 5.7 6.2 5.1 5.7 6.3 5.8 7.1 6.3 6.5 7.6 4.9 7.3 ## [109] 6.7 7.2 6.5 6.4 6.8 5.7 5.8 6.4 6.5 7.7 7.7 6.0 6.9 5.6 7.7 6.3 6.7 7.2 ## [127] 6.2 6.1 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8 ## [145] 6.7 6.7 6.3 6.5 6.2 5.9  Reshaping reshape2::melt(table(diamonds$cut, diamonds$color), var = c(\u0026quot;cut\u0026quot;, \u0026quot;color\u0026quot;))  ## cut color value ## 1 Fair D 163 ## 2 Good D 662 ## 3 Very Good D 1513 ## 4 Premium D 1603 ## 5 Ideal D 2834 ## 6 Fair E 224 ## 7 Good E 933 ## 8 Very Good E 2400 ## 9 Premium E 2337 ## 10 Ideal E 3903 ## 11 Fair F 312 ## 12 Good F 909 ## 13 Very Good F 2164 ## 14 Premium F 2331 ## 15 Ideal F 3826 ## 16 Fair G 314 ## 17 Good G 871 ## 18 Very Good G 2299 ## 19 Premium G 2924 ## 20 Ideal G 4884 ## 21 Fair H 303 ## 22 Good H 702 ## 23 Very Good H 1824 ## 24 Premium H 2360 ## 25 Ideal H 3115 ## 26 Fair I 175 ## 27 Good I 522 ## 28 Very Good I 1204 ## 29 Premium I 1428 ## 30 Ideal I 2093 ## 31 Fair J 119 ## 32 Good J 307 ## 33 Very Good J 678 ## 34 Premium J 808 ## 35 Ideal J 896  Variation Continuous variables 5-point summaries summary(iris)  ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ##  ggplot(reshape2::melt(iris), aes(x = variable, y = value)) + geom_boxplot()  ## Using Species as id variables     Note: in the above code, we used the melt() function in reshape2 package to aggregate the data. Explore what melt() does by running it in the console.\nHistograms ggplot(iris, aes(x = Sepal.Length)) + geom_histogram()  ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.     Density plots ggplot(iris, aes(x = Sepal.Length)) + geom_density(fill = \u0026quot;blue\u0026quot;)     Discrete variables Frequency tables table(mpg$class)  ## ## 2seater compact midsize minivan pickup subcompact suv ## 5 47 41 11 33 35 62  prop.table(table(mpg$class))  ## ## 2seater compact midsize minivan pickup subcompact suv ## 0.02136752 0.20085470 0.17521368 0.04700855 0.14102564 0.14957265 0.26495726  Barplots ggplot(mpg, aes(x = reorder(class, class, FUN = length))) + geom_bar() + labs(x = \u0026quot;Class\u0026quot;)     Note: The reorder() function sorts the bars\u0026hellip; the syntax is a bit tricky to understand, so take it as is for now.\nCovariation Continuous variables Scatter plots ggplot(mpg, aes(x = displ, y = hwy)) + geom_point()     Smooth lines ggplot(mpg, aes(x = displ, y = hwy)) + geom_smooth()  ## `geom_smooth()` using method = 'loess' and formula 'y ~ x'     Binning ggplot(mpg, aes(x = displ, y = hwy)) + geom_boxplot(aes(group = cut_width(displ, 0.5)))     Discrete variables Contingency tables table(diamonds$cut, diamonds$color)  ## ## D E F G H I J ## Fair 163 224 312 314 303 175 119 ## Good 662 933 909 871 702 522 307 ## Very Good 1513 2400 2164 2299 1824 1204 678 ## Premium 1603 2337 2331 2924 2360 1428 808 ## Ideal 2834 3903 3826 4884 3115 2093 896  t(table(diamonds$cut, diamonds$color))  ## ## Fair Good Very Good Premium Ideal ## D 163 662 1513 1603 2834 ## E 224 933 2400 2337 3903 ## F 312 909 2164 2331 3826 ## G 314 871 2299 2924 4884 ## H 303 702 1824 2360 3115 ## I 175 522 1204 1428 2093 ## J 119 307 678 808 896  Count plots ggplot(data = diamonds) + geom_count(mapping = aes(x = cut, y = color))     Heat maps dat \u0026lt;- reshape2::melt(table(diamonds$cut, diamonds$color), var = c(\u0026quot;cut\u0026quot;, \u0026quot;color\u0026quot;)) ggplot(dat, aes(x = color, y = cut)) + geom_tile(mapping = aes(fill = value))     Continuous x discrete Faceting ggplot(mpg, aes(x = displ, y = hwy)) + geom_point() + facet_wrap(~ class, nrow = 2)     Adding additional aesthetics ggplot(mpg, aes(x = displ, y = hwy, col = class)) + geom_point()     ","date":1563292800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"bf04761d520cce441a24f994d051ea45","permalink":"https://haziqj.ml/teaching/datascience/3-r4ds/","publishdate":"2019-07-17T00:00:00+08:00","relpermalink":"/teaching/datascience/3-r4ds/","section":"teaching","summary":"As usual, before starting, load all the packages you need.\nlibrary(tidyverse)  ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──  ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.","tags":null,"title":"Useful data science functions in R","type":"book"},{"authors":null,"categories":null,"content":"Assignment You are tasked with conducting an exploratory data analysis of the Kiva.org loans data set. In particular, focus on the following tasks:\n  Investigate the relationship between the loan taker\u0026rsquo;s ability to pay and the usage of the loan (what it was funded for).\n  Preliminary analysis of predictive risk for new clients. What sort of characteristics are associated with high defaulting loans?\n  Explore interesting statistics segregated by regions, e.g. in Asia, what are loans most used for?\n  Can it be said that the Kiva.org loans make a positive impact to society? Perhaps, a region which sees a large number of approved microloans will measure better on a certain metric (e.g. unemployment rate) than in areas do not?\n  Is there any evidence of gender discrepancy in the number or amount of loans being funded?\n  Instructions For each of the questions above, think about\n What data do you need? What transformations and summaries do you need to perform on the data? What kind of plots do you want to visualise?  Prepare an R script file containing the R code that you used for all of your exploratory analysis of the data set, including importing, transforming, visualising, and any statistical summaries of the data.\nThe R script file should be clearly anotated with your comments, such that the next person who reads your R script understands the intent behind your code.\nMake use of commenting to write down any notes or conclusions that you have regarding the analyses.\nData set The main data set that you will have at your disposal are the four .csv files from Kaggle. You might find that there are data that you require for your analysis that is not present in the data set. Try to find additional sources of data from the web for your needs.\nPresenting your work Submit your R script to me for evaluation. The fourth lecture of this course will be dedicated to a group discussion of the five exploratory data analysis tasks above.\n","date":1563231600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"386beae0d58efd8e84b3ec85da334eac","permalink":"https://haziqj.ml/teaching/datascience/4-assignment/","publishdate":"2019-07-16T00:00:00+01:00","relpermalink":"/teaching/datascience/4-assignment/","section":"teaching","summary":"Assignment You are tasked with conducting an exploratory data analysis of the Kiva.org loans data set. In particular, focus on the following tasks:\n  Investigate the relationship between the loan taker\u0026rsquo;s ability to pay and the usage of the loan (what it was funded for).","tags":null,"title":"Kiva.org assignment","type":"book"},{"authors":["Aida Zaini","Haziq Jamil","Elvynna Leong"],"categories":[],"content":"","date":1632873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"10592931ed9eb2bb88c0076e841e5cf4","permalink":"https://haziqj.ml/publication/zaini-2021-investigating/","publishdate":"2021-09-06T14:10:51.579541Z","relpermalink":"/publication/zaini-2021-investigating/","section":"publication","summary":"Hypothesis testing is an essential tool among researchers and practitioners alike, with its use being being widely taught in many a programme at university level. However, past studies have shown that students hold misconceptions about important statistical concepts. This present study aims to reconfirm past efforts in this area, specifically in a South East Asian higher education institution. To test how well undergraduate university students' understood key concepts in hypothesis testing, an online multiple choice questionnaire was deployed. The questionnaire also asked for students' confidence ratings for each question, allowing us to distinguish the confident versus non-confident incorrect responses. A follow-up interview was then conducted to give deeper insights into reasons behind respondents' errors. The main finding is that there are significantly more confident wrong answers than non-confident ones -- highly indicative of the presence of misconceptions among respondents. Among them, students firmly believed that statistical inference procedures provide a direct calculational proof of the null hypothesis. Additionally, students have difficulty formulating correct hypotheses to be tested, and have poor grasp of the role of signficance levels in hypothesis testing. Whether or not students were taking a quantitative-focused programme, or had prior statistics training, had no bearing on their survey score. Despite this, confidence ratings were significantly higher in both groups.","tags":[],"title":"I think I understand: Investigating misconceptions regarding hypothesis test concepts among university students","type":"publication"},{"authors":["Haziq Jamil","Huda M. Ramli","Elvynna Leong"],"categories":[],"content":"","date":1628208e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"1cb2055ce73212dd6fba88285485007e","permalink":"https://haziqj.ml/publication/jamil-advocating-2022/","publishdate":"2021-09-06T14:10:50.616027Z","relpermalink":"/publication/jamil-advocating-2022/","section":"publication","summary":"Institutional mathematics education has long been traditional in its ways of being teacher-centric, a tradition which perhaps dates back to the Ancient Greece. Much like the society in those days, where there was a wary public feeling about the rigidness of the mathematical instruction in Pythagoras' school, mathematics educators find themselves in a similar position in the common era of 2020. Unlike the Ancient Greece however, the battle is for the sustained delivery of a comprehensive mathematics education in the midst of the Covid-19 pandemic. It would be fair to say that mathematics departments across all levels of the education sector have been affected drastically; more so on instructors who favour the traditional \"chalk and talk\" method of instruction. In this article, we share several lessons learned in the delivery of mathematical instruction at undergraduate university level during the Covid-19 pandemic, drawing on our experience at Universiti Brunei Darussalam. These include specific methods for implementing online learning effectively, the pros and cons of such methods, and how we can use computer based tools to make learning more conducive. We highly think that these implementations are beneficial to be adapted by mathematics departments anywhere as a means of adapting to the new realities post Covid-19.","tags":["Automated assessments","Blended learning","Dynamic exercises","Learning management systems"],"title":"Advocating Blended Learning for University Undergraduate Level Mathematical Instruction Beyond Covid-19","type":"publication"},{"authors":["Myrsini Katsikatsou","Irini Moustaki","Haziq Jamil"],"categories":[],"content":"","date":1618444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"0b0af8c523f64c5ef00ea11a3fe79ba3","permalink":"https://haziqj.ml/publication/katsikatsou-pairwise-2021/","publishdate":"2021-09-06T14:10:51.180368Z","relpermalink":"/publication/katsikatsou-pairwise-2021/","section":"publication","summary":"Methods for the treatment of item non-response in attitudinal scales and in large-scale assessments under the pairwise likelihood (PL) estimation framework and under a missing at random (MAR) mechanism are proposed. Under a full information likelihood estimation framework and MAR, ignorability of the missing data mechanism does not lead to biased estimates. However, this is not the case for pseudo-likelihood approaches such as the PL. We develop and study the performance of three strategies for incorporating missing values into confirmatory factor analysis under the PL framework, the complete-pairs (CP), the available-cases (AC) and the doubly robust (DR) approaches. The CP and AC require only a model for the observed data and standard errors are easy to compute. Doubly-robust versions of the PL estimation require a predictive model for the missing responses given the observed ones and are computationally more demanding than the AC and CP. A simulation study is used to compare the proposed methods. The proposed methods are employed to analyze the UK data on numeracy and literacy collected as part of the OECD Survey of Adult Skills.","tags":["Composite likelihood","Latent variable models"],"title":"Pairwise likelihood estimation for confirmatory factor analysis models with categorical variables and data that are missing at random","type":"publication"},{"authors":["Artemis Koukounari","Haziq Jamil","Elena Erosheva","Clive Shiff","Irini Moustaki"],"categories":[],"content":"Author Summary Accurate schistosomiasis diagnosis is essential to assess the impact of large scale and repeated mass drug administration to control or even eliminate this disease. However, in schistosomiasis diagnostic studies, several inherent study design issues pose a real challenge for the currently available statistical tools used for diagnostic modelling and associated data analysis and conclusions. More specifically, those study design issues are:\n the inclusion of small number of diagnostic tests (i.e. most often five), non formal consensus about a schistosomiasis gold standard, the contemporary use of relatively small sample sizes in relevant studies due to lack of research funding, the differing levels of prevalence of the studied disease even within the same area of one endemic country and other real world factors such as: the lack of appropriate equipment, the variability of certain methods due to biological phenomena and training of technicians across the endemic countries because of scarce financial resources contributing to the existing lack of a schistosomiasis gold standard. The current study aims to caution practitioners from blindly applying statistical models with small number of diagnostic tests and sample sizes, proposing design guidelines of future schistosomiasis diagnostic accuracy studies with recommendations for further research. While our study is centred around the diagnosis of schistosomiasis, we feel that the recommendations can be adapted to other major tropical infectious diseases as well.  ","date":1612396800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"12351fa741562a2a5a3a5655384bab57","permalink":"https://haziqj.ml/publication/koukounari-latent-2021/","publishdate":"2021-09-06T14:10:51.313481Z","relpermalink":"/publication/koukounari-latent-2021/","section":"publication","summary":"Various global health initiatives are currently advocating the elimination of schistosomiasis within the next decade. Schistosomiasis is a highly debilitating tropical infectious disease with severe burden of morbidity and thus operational research accurately evaluating diagnostics that quantify the epidemic status for guiding effective strategies is essential. Latent class models (LCMs) have been generally considered in epidemiology and in particular in recent schistosomiasis diagnostic studies as a flexible tool for evaluating diagnostics because assessing the true infection status (via a gold standard) is not possible. However, within the biostatistics literature, classical LCM have already been criticised for real-life problems under violation of the conditional independence (CI) assumption and when applied to a small number of diagnostics (i.e. most often 3-5 diagnostic tests). Solutions of relaxing the CI assumption and accounting for zero-inflation, as well as collecting partial gold standard information, have been proposed, offering the potential for more robust model estimates. In the current article, we examined such approaches in the context of schistosomiasis via analysis of two real datasets and extensive simulation studies. Our main conclusions highlighted poor model fit in low prevalence settings and the necessity of collecting partial gold standard information in such settings in order to improve the accuracy and reduce bias of sensitivity and specificity estimates.","tags":["Latent Class Models"],"title":"Latent Class Analysis: Insights about design and analysis of schistosomiasis diagnostic studies","type":"publication"},{"authors":["Haziq Jamil"],"categories":null,"content":"Links  An interactive visualisation of Bayesian inference MCMC demo  References  Bollen, K. A. (1989). Structural Equations with Latent Variables. John Wiley \u0026amp; Sons. ISBN: 978-0-471-01171-2. DOI: 10.1002/9781118619179. Denwood, M. (2016). runjags: An R Package Providing Interface Utilities, Model Templates, Parallel Computing Methods and Additional Distributions for MCMC Models in JAGS. Journal of Statistical Software 71.9, pp. 1–25. DOI: 10.18637/jss.v071.i09. Edwards, J. R. and R. P. Bagozzi (2000). On the nature and direction of relationships between constructs and measures. Psychological methods 5.2, p. 155. Hafez, M. S., I. Moustaki, and J. Kuha (2015). Analysis of multivariate longitudinal data subject to nonrandom dropout. Structural Equation Modeling: A Multidisciplinary Journal 22.2, pp. 193–201. Kruschke, J. K. (2014). Doing Bayesian Data Analysis: A tutorial with R, JAGS, and Stan. Second Edition. Boston: Academic Press. ISBN: 978-0-12-405888-0. Rabe-Hesketh, S. and A. Skrondal (2008). Multilevel and longitudinal modeling using Stata. STATA press.  Sample code in JAGS model{ for (i in 1:N) { for (k in 1:p_phy) { # Physical y_phy[i,k] ~ dnorm(a_phy[k] + b_phy[k] * Phy[i], 1/v_phy[k]) } for (k in 1:p_cgn) { # Cognitive y_cgn[i,k] ~ dnorm(a_cgn[k] + b_cgn[k] * Cgn[i], 1/v_cgn[k]) } for (k in 1:p_cmb) { # Combatant y_cmb[i,k] ~ dnorm(a_cmb[k] + b_cmb[k] * Cmb[i], 1/v_cmb[k]) } Phy[i] ~ dnorm(a_Phy + b_Phy * Abl[i], 1/v_Phy) Cgn[i] ~ dnorm(a_Cgn + b_Cgn * Abl[i], 1/v_Cgn) Cmb[i] ~ dnorm(a_Cmb + b_Cmb * Abl[i], 1/v_Cmb) Abl[i] ~ dnorm(a_Abl, 1/v_Abl) } # Priors --------------------------------------------------------------------- a_phy[1] ~ dnorm(0,1e-3) b_phy[1] \u0026lt;- 1 for(k in 2:p_phy) { a_phy[k] ~ dnorm(0,1e-3) b_phy[k] ~ dnorm(0,1e-2) } for(k in 1:p_phy) { sd_phy[k] ~ dgamma(1,.5) v_phy[k] \u0026lt;- pow(sd_phy[k],2) } a_cgn[1] ~ dnorm(0,1e-3) b_cgn[1] \u0026lt;- 1 for(k in 2:p_cgn) { a_cgn[k] ~ dnorm(0,1e-3) b_cgn[k] ~ dnorm(0,1e-2) } for(k in 1:p_cgn) { sd_cgn[k] ~ dgamma(1,.5) v_cgn[k] \u0026lt;- pow(sd_cgn[k],2) } a_cmb[1] ~ dnorm(0,1e-3) b_cmb[1] \u0026lt;- 1 for(k in 2:p_cmb) { a_cmb[k] ~ dnorm(0,1e-3) b_cmb[k] ~ dnorm(0,1e-2) } for(k in 1:p_cmb) { sd_cmb[k] ~ dgamma(1,.5) v_cmb[k] \u0026lt;- pow(sd_cmb[k],2) } a_Phy \u0026lt;- 0 a_Cgn \u0026lt;- 0 a_Cmb \u0026lt;- 0 a_Abl \u0026lt;- 0 b_Phy \u0026lt;- 1 b_Cgn ~ dnorm(0,1e-2) b_Cmb ~ dnorm(0,1e-2) sd_Phy ~ dgamma(1,.5) v_Phy \u0026lt;- pow(sd_Phy,2) sd_Cgn ~ dgamma(1,.5) v_Cgn \u0026lt;- pow(sd_Cgn,2) sd_Cmb ~ dgamma(1,.5) v_Cmb \u0026lt;- pow(sd_Cmb,2) sd_Abl ~ dgamma(1,.5) v_Abl \u0026lt;- pow(sd_Abl,2) }  ","date":1605796200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"f7b045ccf22c88e9f0021ae8578aad0a","permalink":"https://haziqj.ml/talk/a-latent-variable-model-for-maximal-performance-testing-with-dropouts-for-military-applications/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/a-latent-variable-model-for-maximal-performance-testing-with-dropouts-for-military-applications/","section":"event","summary":"Soldiers are expected to perform complex and demanding tasks during operations, often while carrying a heavy load. It is therefore important for commanders to understand the relationship between load carriage and soldiers’ performance, as such knowledge helps inform decision-making on training policies, operational doctrines, and future soldier systems requirements. In order to investigate this, repeated experiments were conducted to capture key soldier performance parameters under controlled conditions. The data collected was found to contain missing values due to dropouts as well as non-measurement. We propose a Bayesian structural equation model to quantify a latent variable representing soldiers’ abilities, while taking into consideration the non-random nature of the dropouts and time-varying effects. This talk describes the modelling exercise conducted, emphasising the statistical model-building process as well as the practical reporting of the outputs of the model.","tags":["Latent Variable Models","Structural Equation Models"],"title":"A latent variable model for maximal performance testing with dropouts for military applications","type":"event"},{"authors":["Haziq Jamil","Wicher Bergsma"],"categories":[],"content":"","date":1604793600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"4bc150beab57160ff87735d5ba38598b","permalink":"https://haziqj.ml/publication/jamil-bayesian-2021/","publishdate":"2021-09-06T14:10:50.903444Z","relpermalink":"/publication/jamil-bayesian-2021/","section":"publication","summary":"The Bayesian approach to modelling differs from the frequentist approach primarily in the supplementation of additional information about the parameters to the data. If we specify a \"good\" prior, in the sense that the prior nudges the likelihood in the right direction, then the estimates will also be good. This is what we aim to do in the case of variable selection problems, whereby the Bayesian method reduces the selection problem to one of estimation from a true search of the variable space for the model which optimises a certain criterion. We contribute to the vastly available literature of variable selection methods by using I-priors [(Bergsma, 2019)](https://doi.org/10.1016/j.ecosta.2019.10.002)---a class of Gaussian distributions which has the distinguishing property of having covariance proportional to the Fisher information (of the model parameters). The original motivation behind the I-prior methodology was to develop a novel unifying approach to various regression models. In this work, we detail the I-prior model used, and showcase some simulation results and several real-world applications in which the I-prior performs favourably compared to other prior distributions and/or variable selection techniques in terms of model size, $R^2$, predictive ability, and so on.","tags":["Bayesian","Collinearity","Linear regression","MCMC","Variable selection"],"title":"Bayesian Variable Selection for Linear Models Using I-Priors","type":"publication"},{"authors":["Wicher Bergsma","Haziq Jamil"],"categories":[],"content":"","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"3a07da07eaf0581dae090bf0b3377086","permalink":"https://haziqj.ml/publication/bergsma-regression-2020/","publishdate":"2021-09-06T14:10:51.03903Z","relpermalink":"/publication/bergsma-regression-2020/","section":"publication","summary":"We introduce the I-prior methodology as a unifying framework for estimating a variety of regression models, including varying coefficient, multilevel, longitudinal models, and models with functional covariates and responses. It can also be used for multi-class classification, with low or high dimensional covariates. The I-prior is generally defined as a maximum entropy prior. For a regression function, the I-prior is Gaussian with covariance kernel proportional to the Fisher information on the regression function, which is estimated by its posterior distribution under the I-prior. The I-prior has the intuitively appealing property that the more information is available on a linear functional of the regression function, the larger the prior variance, and the smaller the influence of the prior mean on the posterior distribution. Advantages compared to competing methods, such as Gaussian process regression or Tikhonov regularization, are ease of estimation and model comparison. In particular, we develop an EM algorithm with a simple E and M step for estimating hyperparameters, facilitating estimation for complex models. We also propose a novel parsimonious model formulation, requiring a single scale parameter for each (possibly multidimensional) covariate and no further parameters for interaction effects. This simplifies estimation because fewer hyperparameters need to be estimated, and also simplifies model comparison of models with the same covariates but different interaction effects; in this case, the model with the highest estimated likelihood can be selected. Using a number of widely analyzed real data sets we show that predictive performance of our methodology is competitive. An R-package implementing the methodology is available (Jamil, 2019).","tags":["reproducing kernel","RKHS","RKKS","Fisher information","objective prior","empirical Bayes"],"title":"Regression modelling with I-priors: With applications to functional, multilevel and longitudinal data","type":"publication"},{"authors":["Haziq Jamil"],"categories":null,"content":"","date":1590670800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"13804e4a317d12322b6406fcc412a9b4","permalink":"https://haziqj.ml/talk/investigating-the-effect-of-load-carriage-on-soldiers-performances-using-structural-equation-models/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/investigating-the-effect-of-load-carriage-on-soldiers-performances-using-structural-equation-models/","section":"event","summary":"Soldiers are required to perform tasks that call upon a complex combination of their physical and cognitive capabilities. For example, soldiers are expected to communicate effectively with each other, operate specialised equipment, and maintain overall situational awareness--often while carrying a heavy load. From a planning and doctrine perspective, it is important for commanders to understand the relationship between load carriage and soldiers’ performance. Such information could help provide recommendations in advising future policies on training, operational safety, and future soldier systems requirements. To this end, the Royal Brunei Armed Forces (RBAF) conducted controlled experiments and collected numerous measurements intended to capture key soldier performance parameters. The structure of the data set provided several interesting challenges, namely 1) how do we define “performance”?; 2) how do we appropriately take into account the longitudinal nature of the data (repeated measurements)?; and 3) how do we handle non-ignorable dropouts? We propose a structural equation model to quantify a latent variable representing soldiers' abilities, while taking into consideration the non-random nature of the dropouts and time-varying effects. The main output of the study is to quantify the relationship between load carried versus performance. Additionally, modelling the dropouts allow us to also determine “expected time to exhaustion” for a given load carried by a soldier.","tags":["Latent Variable Models","Structural Equation Models"],"title":"Investigating the effect of load carriage on soldiers’ performances using structural equation models","type":"event"},{"authors":null,"categories":null,"content":"","date":1584316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"1cda19dd8843ec96b438455bdcdcd736","permalink":"https://haziqj.ml/project/stat-tables/","publishdate":"2020-03-16T00:00:00Z","relpermalink":"/project/stat-tables/","section":"project","summary":"An open source PDF format of statistical tables for use in examinations, tests, assignments, and so on.","tags":["Statistical Tables","Misc"],"title":"Statistical Tables","type":"project"},{"authors":["Haziq Jamil","Ayesha Salleh","Le Thieng Chan"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"de3a34366f79cf5c9467f26a51be5443","permalink":"https://haziqj.ml/publication/jamil-2020-investigating/","publishdate":"2021-09-06T14:10:51.723746Z","relpermalink":"/publication/jamil-2020-investigating/","section":"publication","summary":"Soldiers are required to perform tasks that call upon a complex combination of their physical and cognitive capabilities. For example, soldiers are expected to communicate effectively with each other, operate specialised equipment, and maintain overall situational awareness---often while carrying a heavy load. From a planning and doctrine perspective, it is important for commanders to understand the relationship between load carriage and soldiers' performance. Such information could help provide recommendations in advising future policies on training, operational safety, and future soldier systems requirements. To this end, the Royal Brunei Armed Forces (RBAF) conducted controlled experiments and collected numerous measurements intended to capture key soldier performance parameters. The structure of the data set provided several interesting challenges, namely 1) How does one define ``performance''?; 2) How do we handle non-ignorable dropouts?; and 3) How do we appropriately take into account the longitudinal nature of the data (repeated measurements)? We propose a structural equation model to quantify a latent variable representing soldiers' abilities, while taking into consideration the non-random nature of the dropouts and time-varying effects. The main output of the study is to quantify the relationship between load carried versus performance. Additionally, modelling the dropouts allow us to also determine expected ``time to exhaustion'' for any given weight carried.","tags":[],"title":"Investigating the effect of load carriage on soldiers' performances using Bayesian structural equation models","type":"publication"},{"authors":["Haziq Jamil","Wicher Bergsma"],"categories":[],"content":"","date":1575072e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"393e88842313b2fd93b654ce2cb3db18","permalink":"https://haziqj.ml/publication/jamil-iprior-2019/","publishdate":"2021-09-06T14:10:50.772372Z","relpermalink":"/publication/jamil-iprior-2019/","section":"publication","summary":"This is an overview of the R package iprior, which implements a unified methodology for fitting parametric and nonparametric regression models, including additive models, multilevel models, and models with one or more functional covariates. Based on the principle of maximum entropy, an I-prior is an objective Gaussian process prior for the regression function with covariance kernel equal to its Fisher information. The regression function is estimated by its posterior mean under the I-prior, and hyperparameters are estimated via maximum marginal likelihood. Estimation of I-prior models is simple and inference straightforward, while small and large sample predictive performances are comparative, and often better, to similar leading state-of-the-art models. We illustrate the use of the iprior package by analysing a simulated toy data set as well as three real-data examples, in particular, a multilevel data set, a longitudinal data set, and a dataset involving a functional covariate.","tags":["Gaussian process regression","objective prior","empirical Bayes","RKHS","EM algorithm","Nystrom method","I-prior"],"title":"iprior: An R Package for Regression Modelling using I-priors","type":"publication"},{"authors":["Haziq Jamil"],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways: - **Create** slides using Wowchemy's [*Slides*](https://wowchemy.com/docs/managing-content/#create-slides) feature and link using `slides` parameter in the front matter of the talk file - **Upload** an existing slide deck to `static/` and link using `url_slides` parameter in the front matter of the talk file - **Embed** your slides (e.g. Google Slides) or presentation video on this page using [shortcodes](https://wowchemy.com/docs/writing-markdown-latex/). Further event details, including [page elements](https://wowchemy.com/docs/writing-markdown-latex/) such as image galleries, can be added to the body of this page. -- ","date":1573653600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"af988344b8d94a14a102df14b3a13841","permalink":"https://haziqj.ml/talk/bayesian-variable-selection-for-linear-models/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/bayesian-variable-selection-for-linear-models/","section":"event","summary":"In statistical modelling, there is often a genuine interest to learn the most reasonable, parsimonious, and interpretable model that fits the data. This is especially true when faced with the oddly perplexing phenomenon of having \"too much information\" (data saturation). Model selection is indeed a vastly covered topic. In this talk, I will focus on the Bayesian approach to model selection, emphasising the selection of variables in a linear regression model. The outcome of the talk is three-fold: 1) To introduce the statistical framework for Bayesian variable selection; 2) to understand how we can use model probabilities as a basis for model selection; and 3) to demonstrate its application using real-world data (mortality and air pollution data). The hope is that the audience will gain an understanding of the method to possibly spur on further research and applications in their respective work.\"\n","tags":null,"title":"Bayesian Variable Selection for Linear Models","type":"event"},{"authors":["Haziq Jamil"],"categories":[],"content":"Introductory Data Science using R Lecture 1: The Data Science Framework\n Admin  Content available at https://haziqj.ml/teaching 4 x 2hr lectures 10min break on the hour Ask questions as we go along   Structure  Lecture 1: The data science framework Lecture 2: Using R Lecture 3: Data science with R Lecture 4: Exploratory analysis of Kiva.org data      The scientific method is an empirical method of acquiring knowledge that has characterized the development of science since at least the 17th century. It involves careful observation, applying rigorous skepticism about what is observed, given that cognitive assumptions can distort how one interprets the observation. It involves formulating hypotheses, via induction, based on such observations; experimental and measurement-based testing of deductions drawn from the hypotheses; and refinement (or elimination) of the hypotheses based on the experimental findings. These are principles of the scientific method, as distinguished from a definitive series of steps applicable to all scientific enterprises   The scientific inquiry data + model \u0026mdash;\u0026gt; understand\n Not new, arises in many fields  Natural sciences Econometrics Psychology Sociology etc.     From a data science perspective, we are interested in the numerical aspects. qualitative vs quantitative It really is not new. Examples?    Giuseppe Piazzi\u0026rsquo;s observations in the Monatliche Correspondenz, September 1801.\n 18th century Collected data on position of a celestial object. Data and model show that the object did not behave like it was supposed to. Announced it as a comet but really was a planet.     Design of experiments; randomised control trials. Sir Ronald Fisher (1890\u0026ndash;1962).   Fisher credited with the methods to analyze these types of data sets ANOVA Note the deliberate intent of collecting data for this specific purpose c.f. surveys    Data is now available by happenstance, and not just collected by design.\n Big Data The more we measure, the more we don\u0026rsquo;t understand\n Breadth vs depth paradox; Big p Small n; The curse of dimensionality \u0026ldquo;Data first\u0026rdquo; paradigm Ethics; privacy   Data collected was manageable and intended. E.g. surveys Computing power Able to quantify greater degree the actions of individuals, but less able to characterize society Data comes after the question. Often do not have the luxury of tailoring what data is collected. Fundamental statistics issues surrounding data are thrown out the window: precision and accuracy. bias in data.    define: Data Science\nThe \u0026ldquo;concept to unify statistics, data analysis, machine learning and their related methods\u0026rdquo; in order to \u0026ldquo;understand and analyze actual phenomena\u0026rdquo; with data.\n Multi-displinary field Goal: extract knowledge and insights from structured and unstructured data   In essence, need a systematic way of dealing with data. Need to combine knowledge from various fields. While every field was working in silos, they specialised in their own thing. Data science unites the fields of stats/maths and computer science to make data actionable.     Examples of Data Science problems Real-world problems from the Alan Turing Institute\n Real-time jammer detection, identification and localization in 3G and 4G networks Automated matching of businesses to government contract opportunities Using real-world data to advance air traffic control Personalised lung cancer treatment modelling using electronic health records and genomics   ATI is the national institute for data science and artificial intelligence. interesting to ponder, why was it named after Alan Turing, the comuting pioneer?    Examples of Data Science problems Real-world problems from the Alan Turing Institute\n Identify potential drivers of engaging in extremism News feed analysis to help understand global instability Improved strength training using smart gym equipment data    Scope: Exploratory     Focus on transform and visualise Modelling requires a specific skill set (Stats or ML) GOAL: Generate many promising leads that you can later explore in more depth   Machine Learning vs Statistics Statistics aims to turn humans into robots.\n Concept of \u0026ldquo;statistical proof\u0026rdquo; Often interest is inference  Machine learning aims to turn robots into humans.\n Make sense of patterns from big data Often interest is prediction   Statistics aims to remove the bias of humans when perceiving patterns in data sets. Learn not to be conned; when someone tells you it is such, need proof. Stats: How big is big, and is it enough? Measuring effects. Important question: causality? On the other hand ML or AI aims to equip computers with human skills: image understanding, speech recognition, natural language processing, etc. Kind of \u0026ldquo;reverse engineering\u0026rdquo; of world processes based on data that is observed. Generate large labelled data sets from humans. Train models. Interesting note: programming language also speaks as to what your background is. R for stats, Python for ML.    Data Quality and Readiness There\u0026rsquo;s a sea of data, but most of it is undrinkable\nData neglect: data cleaning is tedious and complex\n 80-20 rule of Data Science  Most time is spent cleaning up data Affectionally called data \u0026ldquo;wrangling\u0026rdquo; [TBA] Data Readiness levels (Bands A, B and C)   So much for the world\u0026rsquo;s most sexiest job of the 21st century! according to business harvard review 2012. Company hires ML, software engineers, but not data cleaners! The importance of data is hard to overstate.    Types of data  Structured data  Data is in a nicely organised repository E.g. Tables, matrices, etc.   Unstructured data  Information does not have a predefined data model E.g. images, colours, text, sound, etc.     Types of data  Continuous data  Measurements are taken on a continuous scale e.g. height, weight, temperature, GDP, distance, etc. Usually arises from physical experiments   Discrete data  Measurements which can only take certain values e.g. sex, survey responses (Likert scales), occupation, ratings, ranks, etc. Usually arises in social sciences     Types of data    Treatment Continuous Data Categorical Data     Import class numeric factor, ordinal   Visualise Histograms, density plots, scatter plot, box \u0026amp; whisker plot, pie charts Bar plots,   Summarise 5-point summaries Frequency tables     Exploratory Data Analysis   Generate questions about your data.\n  Search for answers by visualising, transforming, and modelling your data.\n  Use what you learn to refine your questions and/or generate new questions.\n  More on this later\u0026hellip;\n Modelling $$y_i = \\alpha + \\beta x_i + \\epsilon_i$$ $$\\epsilon_i \\sim \\text{N}(0,\\sigma^2)$$\n EDA does not help in providing statistical proof, nor give predictions To do this, engage in statistical or ML models Many types of models, depending on what question you want answered   The R programming language R is a language and environment for statistical computing and graphics https://www.r-project.org/about.html\n It is free and open source Runs everywhere Supports extensions Engaging community Links to other languages       ggplot2 in R     ggplot2 in R     ggplot2 in R     Kiva.org data set https://www.kaggle.com/kiva/data-science-for-good-kiva-crowdfunding#kiva_loans.csv\n Exercise  What exploratory analyses would you conduct on this data set? What other data do you need to supplement your analyses? What questions do you aim to answer?   End of Lecture 1 Questions?\n Supplementary material  Inference vs Prediction Source: datascienceblog.com\n Inference: Use the model to learn about the data generation process. Prediction: Use the model to predict the outcomes for new data points.    Model interpretability  Model interpretability is necessary for inference In a nutshell, a model is interpretable if we can \u0026ldquo;see\u0026rdquo; how the model generates its estimates c.f. Blackboxes Interpretable models often uses simplified assumptions   Inference: Use the model to learn about the data generation process. Prediction: Use the model to predict the outcomes for new data points.    Model complexity  A complex model is often better at prediction tasks \u0026ldquo;More parameters to tune\u0026rdquo; However, model interpretability suffers   Bias-Variance tradeoff $$ E[f(x) - \\hat f (x)]^2 = \\text{Bias}^2[\\hat f(x)] + \\text{Var}[\\hat f(x)] + \\sigma^2 $$\n Bias: How close to the truth Variance: How varied the predictions will be under a new data set    Linear regression Economic freedom = 2.6 + 0.6 Trade\n Trade: tariffs, regulatory trade barriers, black market, control movement of capital and people, trade    Neural networks Source: towardsdatascience.com\n Survey Methodology Source: Groves et al. (2009)\n Three populations  Sampling design for BSA survey  Target: Adults aged 18 or over in GB Survey: Private households south of the Caledonian Canal Frame: Addresses in the Postcode address file  Multistage design:\n Stratify by postcode sectors Simple random sampling of addresses Simple random sampling of individuals  From 60mil people, obtained 3,297 respondents in final sample.\n What is random? Not predetermined Everyone should be able to be sampled with positive probability Unbiased    See also  https://www.surveymonkey.com/mp/sample-size-calculator/ http://www.bsa.natcen.ac.uk/latest-report/british-social-attitudes-30/technical-details.aspx   Data Readiness  Band C: Hearsday data. Is it really available? Has it actually been recorded? Format: PDF, log books, etc. Band B: Ready for exploratory analysis, visualisations. Missing values, anomalies, \u0026hellip; Band A: Ready for ML/Stats models.   One  **Two**  Three  --- A fragment can accept two optional parameters: - `class`: use a custom style (requires definition in custom CSS) - `weight`: sets the order in which a fragment appears --- ## Speaker Notes Add speaker notes to your presentation ```markdown {{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} ``` Press the `S` key to view the speaker notes!  Only the speaker can read these notes Press S key to view   --- ## Themes - black: Black background, white text, blue links (default) - white: White background, black text, blue links - league: Gray background, white text, blue links - beige: Beige background, dark text, brown links - sky: Blue background, thin dark text, blue links --- - night: Black background, thick white text, orange links - serif: Cappuccino background, gray text, brown links - simple: White background, black text, blue links - solarized: Cream-colored background, dark green text, blue links --- ## Custom Slide Customize the slide style and background ```markdown {{}} {{}} {{}} ``` --- ## Custom CSS Example Let's make headers navy colored. Create `assets/css/reveal_custom.css` with: ```css .reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } ``` --- # Questions? [Ask](https://discourse.gohugo.io) [Documentation](https://sourcethemes.com/academic/docs/) --  ","date":1562630400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"89ac91e20810a6d449048de588e2b351","permalink":"https://haziqj.ml/slides/teach-ds-1/","publishdate":"2019-07-09T00:00:00Z","relpermalink":"/slides/teach-ds-1/","section":"slides","summary":"Learn about the data science framework, including importing, summarising, and visualising data using R.","tags":[],"title":"Introductory Data Science using R","type":"slides"},{"authors":["Haziq Jamil"],"categories":[],"content":"Introductory Data Science using R R Exercise: The birthday problem\n In a room of 23 people, what is the probability that at least two people share the same birthday?\n Let\u0026rsquo;s count First, some assumptions:\n There are only 365 days in a year Every day is equally likely to be a birthday Everyone\u0026rsquo;s birthday is independent of each other  Strategy: It\u0026rsquo;s easier to figure out the probability of the complementary event. $$P(A) = 1 - P(A^c)$$ \n What\u0026rsquo;s the complement?  Let $A$ = At least two people share the same birthday Then $A^c$ = Nobody shares any birthday (all birthdays are different) Label the individuals from $1,\\dots,23$ How many possible birthdays can person 1 have? 365 out of 365 How many possible birthdays can person 2 have? 364 out of 365 \u0026hellip;   What\u0026rsquo;s the complement?  Since all events are independent, $$P(A^c) = \\frac{365}{365} \\times \\frac{364}{365} \\times \\cdots \\times \\frac{365-23+1}{365}$$ $$= \\frac{365!}{(365-23)!365^{23}}$$ Thus, $$P(A) = 1 - \\frac{365!}{(365-23)!365^{23}}$$   Logarithms Factorials are often too large to compute and can cause memory overflow. Adopt the alternative formula\n$$P(A) = 1 - \\exp \\big\\{ \\log(365!) - \\log((365-23)!) $$ $$- 23 \\log 365 \\big\\}$$\n Write this in R Functions that you need:\n factorial() to compute factorials lfactorial() to compute log factorials exp() to compute exponentials   New question In a room of $x$ people, what is the probability that at least two people share the same birthday?\n Write this in R Write a function that takes a positive integer x and returns the probability that at least two people share the same birthday.\nBONUS: Plot it!\n","date":1562544e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"a1180d9965201e1f19581c591b971ef0","permalink":"https://haziqj.ml/slides/teach-ds-2/","publishdate":"2019-07-08T00:00:00Z","relpermalink":"/slides/teach-ds-2/","section":"slides","summary":"Part 2--Learn about the data science framework, including importing, summarising, and visualising data using R.","tags":[],"title":"Getting started with R","type":"slides"},{"authors":null,"categories":null,"content":"Truncated normal distributions often appear in the estimation of various important statistical models, for example, binary and multinomial probit models, tobit models, and constrained linear regresion. Often times, a substantial part of the estimation algorithm involves the computation of moments or probabilities of the truncated normal distribution. It is therefore important that the computational backend for producing these values of interest is made as efficient as possible.\nAmong other things, it would be interesting to look at the following:\n Efficient ways to obtain moments $\\text{E}[g(X)]$ where $X$ is distributed according to a truncated normal distribution. Efficient ways to obtain probabilities involving truncated normal distributions. Univariate and multivariate truncated normal distributions.  The main goal for this project is to create an R package which can be used by other developers and researchers for their specific usage.\nWork involved: literature review and survey of current methods, identifying bottlenecks in algorithms, propose methodology to overcome such bottlenecks, software design.\n","date":1552089600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"d674c834b27da4107efdc00bd55b4060","permalink":"https://haziqj.ml/future-project/truncated-normal/","publishdate":"2019-03-09T00:00:00Z","relpermalink":"/future-project/truncated-normal/","section":"future-project","summary":"To explore efficient computational methods for evaluating truncated normal distributions.","tags":["Truncated Normal","R Package"],"title":"Efficient methods for truncated normal distributions","type":"future-project"},{"authors":null,"categories":null,"content":"Often, key constructs of interest remain elusive in a quantitative study because they are impossible to measure directly. Examples from social science studies include individuals' intelligence quotient (IQ), or political tendencies (left or right). However, under a reflective measurement theory, we can propose that these constructs influence the outcome of several tests which can be measured directly (e.g., IQ can be measured by appropriate IQ tests, and political tendencies by appropriate survey questions).\nTaking this cue from the social sciences, it would be interesting to see this type of methodology being applied to sports and fitness. Spefically, this study comes from a performance optimisation standpoint. Under what conditions do athletes perform best? Athletes' performance can be considered to be latent in nature, but several test items can be constructed to measure the latent variable indirectly.\nIt would also be interesting to construct a non-parametric relationship between several explanatory variables and the latent variable of interest in the structural part of the model. Doing so would allow for better flexibility and predictive abilities. One idea would be to include a Gaussian process regression (GPR) in the structural equation model.\nThis project can be applied in nature, but there is also substantive scope to look into methodology. For instance, a writeup of the estimation of such models when there is a GPR would certainly be noteworthy.\n","date":1552089600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"d4bd4b3a24c8d232ac7e34e98b43199d","permalink":"https://haziqj.ml/future-project/latent-physical/","publishdate":"2019-03-09T00:00:00Z","relpermalink":"/future-project/latent-physical/","section":"future-project","summary":"To devise a suitable statistical model to produce scores for individuals' abilities relevant in the field of fitness and sports.","tags":["Latent Variable Models","Structural Equation Models"],"title":"Scoring individual abilities using multilevel latent variable models","type":"future-project"},{"authors":null,"categories":null,"content":"For my PhD project, I modelled the probabilities of farm houses contracting a particular type of bovine tuberculosis (BTB) over time in Cornwall, UK. I thought something similar could be done specifically for Bruneian data, but not necessarily BTB data of course. Some ideas include, but are not limited to:\n Economic data (income, unemployment, etc.) Educattional data (student-teach ratio, percentage passes in exams, etc.) Demographic data (fertility rate, mortality rate, etc.) Business data (number of coffee shops, land prices, etc.)  The challenge here firstly would be first to acquire data for analysis, which is not readily available. There are official sources for some of these data, but for unstructured data which may be scattered all over the internet and social media, it would be interesting to come up with a device that can scrape these types of data automatically.\nSecondly, at a minimum, this project involves visualisation of the data set. Since these are also temporal data (over time), animations play a vital role here.\nThirdly, appropriate models are to be constructed for prediction of future time values or spatial points for which we have no data on. I would propose looking at some machine learning techniques for this part.\n","date":1552089600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"9ece5ee207ae8c38386ae5636e540eaa","permalink":"https://haziqj.ml/future-project/spatio-temporal-brunei/","publishdate":"2019-03-09T00:00:00Z","relpermalink":"/future-project/spatio-temporal-brunei/","section":"future-project","summary":"A data analytics exercise of patterns in spatial and temporal distributions of certain Bruneian data (demography, health, economics, business).","tags":["Spatial Models","Machine Learning"],"title":"Spatio-temporal analysis of Brunei data","type":"future-project"},{"authors":["Haziq Jamil"],"categories":null,"content":"","date":1548406800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"76a993897186adf8d59d5e023ff88ed0","permalink":"https://haziqj.ml/talk/misconceptions-in-demography/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/misconceptions-in-demography/","section":"event","summary":"Inspired by the Gapminder project, let's talk about common misconceptions about demography.","tags":["Demography"],"title":"Misconceptions in Demography","type":"event"},{"authors":["Haziq Jamil"],"categories":null,"content":"","date":1543933800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"d0bd5ea4ad97ae9b9fb40438b317771a","permalink":"https://haziqj.ml/talk/a-brief-guide-to-variational-inference/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/a-brief-guide-to-variational-inference/","section":"event","summary":"The fitting of complex statistical models that consists of latent or nuisance variables, in addition to various parameters to be estimated, likely involves overcoming an intractable integral. For instance, calculating the likelihood of such models require marginalising over the latent variables, and this may prove to be difficult computationally, either due to dimensionality or model design. Variational inference, or variational Bayes as it is also known, offers an efficient alternative to Markov chain Monte Carlo methods, the Laplace approximation, and quadrature methods. Rooted in Bayesian inference and popularised in machine learning, the main idea is to overcome the difficulties faced by working with “easy” density functions in lieu of the true posterior distribution. The approximating density function is chosen so as to minimise the (reverse) Kullback-Leilber divergence between them. The topics that will be discussed are mean-field distributions, the coordinate ascent algorithm, and approximation properties, with an example following. The hope is that the audience will gain a basic understanding of the method to possibly spur on further research and applications in their respective work.","tags":["Variational Inference"],"title":"A Brief Guide to Variational Inference","type":"event"},{"authors":["Haziq Jamil"],"categories":[],"content":"","date":1538352e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"aede9fe06a89bda5513ec2362c3be962","permalink":"https://haziqj.ml/publication/jamil-2018-phdthesis/","publishdate":"2021-09-06T14:10:51.446134Z","relpermalink":"/publication/jamil-2018-phdthesis/","section":"publication","summary":"Regression analysis is undoubtedly an important tool to understand the relationship between one or more explanatory and independent variables of interest. In this thesis, we explore a novel methodology for fitting a wide range of parametric and nonparametric regression models, called the I-prior methodology [(Bergsma, 2019)](https://doi.org/10.1016/j.ecosta.2019.10.002).\n\nWe assume that the regression function belongs to a reproducing kernel Hilbert or Kreĭn space of functions, and by doing so, allows us to utilise the convenient topologies of these vector spaces. This is important for the derivation of the Fisher information of the regression function, which might be infinite dimensional. Based on the principle of maximum entropy, an I-prior is an objective Gaussian process prior for the regression function with covariance function proportional to its Fisher information.\n\nOur work focusses on the statistical methodology and computational aspects of fitting I-priors models. We examine a likelihood-based approach (direct optimisation and EM algorithm) for fitting I-prior models with normally distributed errors. The culmination of this work is the `R` package [`iprior`](https://cran.r-project.org/package=iprior) which has been made publicly available on CRAN. The normal I-prior methodology is subsequently extended to fit categorical response models, achieved by ''squashing'' the regression functions through a probit sigmoid function. Estimation of I-probit models, as we call it, proves challenging due to the intractable integral involved in computing the likelihood. We overcome this difficulty by way of variational approximations. Finally, we turn to a fully Bayesian approach of variable selection using I-priors for linear models to tackle multicollinearity.\n\nWe illustrate the use of I-priors in various simulated and real-data examples. Our study advocates the I-prior methodology as being a simple, intuitive, and comparable alternative to similar leading state-of-the-art models.\n","tags":[],"title":"Regression modelling using priors depending on Fisher information covariance kernels (I-priors)","type":"publication"},{"authors":null,"categories":null,"content":"","date":153576e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"098d2bd5e0f399d42eef95c1c4c918b2","permalink":"https://haziqj.ml/project/r-iprobit/","publishdate":"2018-09-01T00:00:00Z","relpermalink":"/project/r-iprobit/","section":"project","summary":"Binary and multinomial probit regression using I-priors in `R`.","tags":["R Package","Probit Models"],"title":"R/iprobit","type":"project"},{"authors":["Haziq Jamil"],"categories":null,"content":"","date":1522153800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"d0b440a5660c53de285635aed83f24ad","permalink":"https://haziqj.ml/talk/binary-and-multinomial-regression-using-fisher-information-covariance-kernels-i-priors/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/binary-and-multinomial-regression-using-fisher-information-covariance-kernels-i-priors/","section":"event","summary":"In a regression setting, we define an I-prior as a Gaussian process prior on the regression function with covariance kernel equal to its Fisher information. We present some methodology and computational work on estimating regression functions by working in the appropriate reproducing kernel Hilbert space of functions and assuming an I-prior on the function of interest. In a regression model with normally distributed errors, estimation is simple—maximum likelihood and the EM algorithm is employed. In the classification models (categorical response models), estimation is performed using variational inference. I-prior models perform comparatively well, and often better, to similar leading state-of-the-art models for use in prediction and inference. Applications are plentiful, including smoothing models, modelling multilevel data, longitudinal data, functional covariates, multi-class classification, and even spatiotemporal modelling.","tags":["I-prior","Spatial Models","Probit Models"],"title":"Binary and Multinomial Regression using Fisher Information Covariance Kernels (I-priors)","type":"event"},{"authors":["Haziq Jamil"],"categories":null,"content":"","date":1517488200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"5cbd844c7ec464af0a443cda35bae168","permalink":"https://haziqj.ml/talk/a-beginners-guide-to-variational-inference/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/a-beginners-guide-to-variational-inference/","section":"event","summary":"Estimation of complex models that consists of latent variables and various parameters, in addition to the data that is observed, might involve overcoming an intractable integral. For instance, calculating the likelihood of such models require marginalising over the latent variables, and this may prove to be difficult computationally—either due to model design or dimensionality. Variational inference, or variational Bayes as it is also known, offers an efficient alternative to Markov chain Monte Carlo methods, the Laplace approximation, and quadrature methods. Rooted in Bayesian inference and popularised in machine learning, the main idea is to overcome the difficulties faced by working with “easy” density functions in lieu of the true posterior distribution. The approximating density function is chosen so as to minimise the (reverse) Kullback-Leilber divergence between them. The topics that will be discussed are mean-field distributions, the coordinate ascent algorithm, and its properties, with examples following. The hope is that the audience will gain a basic understanding of the method to possibly spur on further research and applications in their respective work.","tags":["Variational Inference"],"title":"A Beginner's Guide to Variational Inference","type":"event"},{"authors":["Wicher Bergsma","Haziq Jamil"],"categories":null,"content":"","date":1500384600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"99c8b6723dc638fbaf8ac2e5fb948bdd","permalink":"https://haziqj.ml/talk/regression-modelling-with-i-priors/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/regression-modelling-with-i-priors/","section":"event","summary":"This is an overview of a unified methodology for fitting parametric and nonparametric regression models, including additive models, multilevel models, and models with one or more functional covariates. We also discuss an associated R-package called iprior. An I-prior is an objective prior for the regression function, and is based on its Fisher information. The regression function is estimated by its posterior mean under the I-prior, and scale parameters are estimated via maximum marginal likelihood using an Expectation-Maximization (EM) algorithm. Regression modelling using I-priors has several attractive features: it requires no assumptions other than those pertaining to the model of interest; estimation and inference is relatively straightforward; and small and large sample performance can be better than Tikhonov regularization. We illustrate the use of the iprior package by analysing three well- known data sets, in particular, a multilevel data set, a longitudinal data set, and a dataset involving a functional covariate.","tags":["I-prior"],"title":"Regression Modelling with I-Priors","type":"event"},{"authors":["Haziq Jamil"],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways: - **Create** slides using Wowchemy's [*Slides*](https://wowchemy.com/docs/managing-content/#create-slides) feature and link using `slides` parameter in the front matter of the talk file - **Upload** an existing slide deck to `static/` and link using `url_slides` parameter in the front matter of the talk file - **Embed** your slides (e.g. Google Slides) or presentation video on this page using [shortcodes](https://wowchemy.com/docs/writing-markdown-latex/). Further event details, including [page elements](https://wowchemy.com/docs/writing-markdown-latex/) such as image galleries, can be added to the body of this page. -- ","date":1494244800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"7d05004a815d7cff781c0bb94cd2eb74","permalink":"https://haziqj.ml/talk/binary-probit-regression-with-i-priors/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/binary-probit-regression-with-i-priors/","section":"event","summary":"An extension of the I-prior methodology to binary response data is explored. Starting from a latent variable approach, it is assumed that there exists continuous, auxiliary random variables which decide the outcome of the binary responses. Fitting a classical linear regression model on these latent variables while assuming normality of the error terms leads to the well-known generalised linear model with a probit link. A more general regression approach is considered instead, in which an I-prior on the regression function, which lies in some reproducing kernel Hilbert space, is assumed. An I-prior distribution is Gaussian with mean chosen a priori, and covariance equal to the Fisher information for the regression function. By working with I-priors, the benefits of the methodology are brought over to the binary case - one of which is that it provides a unified model-fitting framework that includes additive models, multilevel models and models with one or more functional covariates. The challenge is in the estimation, and a variational approximation is employed to overcome the intractable likelihood. Several real-world examples are presented from analyses conducted in R.","tags":["I-prior","Probit models"],"title":"Binary probit regression with I-priors","type":"event"},{"authors":null,"categories":null,"content":"","date":1486598400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"fd5fe7fd61e8a3c9a98593f9e4a8dc98","permalink":"https://haziqj.ml/project/mmn/","publishdate":"2017-02-09T00:00:00Z","relpermalink":"/project/mmn/","section":"project","summary":"A quantitative text analysis of Brunei's legislative council meeting hansards.","tags":["Quantitative Text Analysis","Misc"],"title":"MMN Brunei quantitative text analysis","type":"project"},{"authors":["Haziq Jamil"],"categories":null,"content":"","date":1478176200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"ecad8bfca13df81f9daea3fb39a116db","permalink":"https://haziqj.ml/talk/i-priors-in-bayesian-variable-selection-from-reproducing-kernel-hilbert-spaces-to-hamiltonian-monte-carlo/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/i-priors-in-bayesian-variable-selection-from-reproducing-kernel-hilbert-spaces-to-hamiltonian-monte-carlo/","section":"event","summary":"I-priors are a class of objective priors for regression functions which makes use of its Fisher information in a function space framework. Currently, I am exploring the use of I-priors in Bayesian variable selection. My talk is a collection of ideas and methods that I picked up along the way in researching my work, in the hopes that it might be of interest and some use in the areas you are working on: 1) Estimation of I-prior models using likelihood methods; 2) The R/iprior package for fitting I-prior models; 3) Shrinkage properties of I-priors and how they link to L2 penalties with individual shrinkage parameters (and equivalently, individual variance hyper-parameters in a Bayesian setting); 4) Estimation of I-prior models in a fully-Bayes setting, with particular interest in the scale parameters; 5) Using Hamiltonian Monte Carlo to obtain better quality MCMC chains for the Bayesian I-prior model. I will also share some information on useful tools and software for reproducible research that I came across during my work, including Shiny apps, GitHub, RStudio (for package development), knitr, and Stan.","tags":["Hamiltonian Monte Carlo"],"title":"I-priors in Bayesian Variable Selection: From Reproducing Kernel Hilbert Spaces to Hamiltonian Monte Carlo","type":"event"},{"authors":null,"categories":null,"content":"I created two shiny apps to explain Hamiltonian Monte Carlo:\n https://haziqj.shinyapps.io/hmc1/ https://haziqj.shinyapps.io/hmc2/  ","date":1478131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"2e5915e2e78f01b515b4f3a17a935081","permalink":"https://haziqj.ml/project/hmc-shiny/","publishdate":"2016-11-03T00:00:00Z","relpermalink":"/project/hmc-shiny/","section":"project","summary":"Shiny apps for explaining Hamiltonian Monte Carlo.","tags":["Hamiltonian Monte Carlo","Misc"],"title":"Hamiltonian Monte Carlo explainer","type":"project"},{"authors":null,"categories":null,"content":"","date":1472342400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"b8ee8da5d3f33ba85e78127cab1e2919","permalink":"https://haziqj.ml/project/r-ipriorbvs/","publishdate":"2016-08-28T00:00:00Z","relpermalink":"/project/r-ipriorbvs/","section":"project","summary":"Bayesian Variable Selection for Linear Models using I-priors in `R`.","tags":["R Package","I-prior","JAGS"],"title":"R/ipriorBVS","type":"project"},{"authors":null,"categories":null,"content":"","date":1471564800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"b2156151337be53d98c41dfb46d06421","permalink":"https://haziqj.ml/project/r-iprior/","publishdate":"2016-08-19T00:00:00Z","relpermalink":"/project/r-iprior/","section":"project","summary":"An `R` package for I-prior regression.","tags":["I-prior","R Package"],"title":"R/iprior","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"0ceb55b6bb1e4423e3ee202ab0b10ece","permalink":"https://haziqj.ml/project/my-phd/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/my-phd/","section":"project","summary":"Regression modelling using priors with Fisher information covariance kernels (I-priors).","tags":["I-prior"],"title":"My PhD Project","type":"project"},{"authors":["Haziq Jamil"],"categories":null,"content":"","date":1447849800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"d01f338f95937b65766443019a42b618","permalink":"https://haziqj.ml/talk/two-stage-bayesian-variable-selection-for-linear-models-using-i-priors/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/two-stage-bayesian-variable-selection-for-linear-models-using-i-priors/","section":"event","summary":"In a previous work, I showed that the use of I-priors in various linear models can be considered as a solution to the over-fitting problem. In that work, estimation was still done using maximum likelihood, so in a sense it was a kind of frequentist-Bayes approach. Switching over to a fully Bayesian framework, we now look at the problem of variable selection, specifically in an ordinary linear regression setting. The appeal of Bayesian methods are that it reduces the selection problem to one of estimation, rather than a true search of the variable space for the model that optimises a certain criterion. I will talk about several Bayesian variable selection methods out there in the literature, and how we can make use of I-priors to improve on results in the presence of multicollinearity.","tags":["I-prior","Bayesian Variable Selection"],"title":"Two-stage Bayesian variable selection for linear models using I-priors","type":"event"},{"authors":["Haziq Jamil"],"categories":null,"content":"","date":1432036800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"f33f9c2ea2696966d8286ac31a1520b2","permalink":"https://haziqj.ml/talk/regression-modelling-using-i-priors/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/regression-modelling-using-i-priors/","section":"event","summary":"The I-prior methodology is a new modelling technique which aims to improve on maximum likelihood estimation of linear models when the dimensionality is large relative to the sample size. By putting a prior which is informed by the dataset (as opposed to a subjective prior), advantages such as model parsimony, lesser model assumptions, simpler estimation, and simpler hypothesis testing can be had. By way of introducing the I-prior methodology, we will give examples of linear models estimated using I-priors. This includes multiple regression models, smoothing models, random effects models, and longitudinal models. Research into this area involve extending the I-prior methodology to generalised linear models (e.g. logistic regression), Structural Equation Models (SEM), and models with structured error covariances.","tags":["I-prior"],"title":"Regression Modelling using I-Priors","type":"event"},{"authors":["Haziq Jamil"],"categories":[],"content":"","date":1277942400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"3b6ef57b619aca3b03214eec7d3940a2","permalink":"https://haziqj.ml/publication/mastersthesis/","publishdate":"2021-09-06T14:10:51.870597Z","relpermalink":"/publication/mastersthesis/","section":"publication","summary":"This dissertation focusses mainly on the Bradley-Terry model and its extensions to in- vestigate three aspects of English Premier League football. Firstly, a comparison of the estimated model rankings with the actual league table will be discussed and how well the model serves as a predictor of the final standings at the end of the season after sev- eral games have been played. Secondly, a home advantage analysis of the teams will be conducted. Thirdly, an estimation of player rankings based on team performances will be attempted. All analyses were conducted in R using the glm() framework, with the exception of the third model, which was specifically coded and solved using an optimi- sation function in R. While the first two analyses generally showed a good fit and clear results, the third one was not as straightforward. The failure to find stationary points in the optimisation problem suggests more work needs to be done to refine the model.","tags":["Bradley-Terry models","football"],"title":"Analysis of paired comparison data using Bradley-Terry models with applications to football data","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://haziqj.ml/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/","section":"","summary":"","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://haziqj.ml/post/getting-started/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://haziqj.ml/slides/example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"","tags":null,"title":"","type":"slides"},{"authors":null,"categories":null,"content":"If the scheduling tool does not load, follow this link to my Calendly scheduling page.\n window.location.href = \"http://example.com\"  Page Redirection   If you are not redirected automatically, follow this link to example.   --   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631006475,"objectID":"b425770b72894590d76a0fbdbc3b13bd","permalink":"https://haziqj.ml/appointment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/appointment/","section":"","summary":"Book an appointment using the Calendly tool.","tags":null,"title":"Book an appointment","type":"page"}]